{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Scandinavian Embedding Benchmark","text":"<p>This is the documentation for the Scandinavian Embedding Benchmark. This benchmark is intended to evaluate the sentence/documents embeddings of large language models.</p> <p>Intended uses for this benchmark:</p> <ul> <li>Evaluating document embeddings of Scandinavian language models</li> <li>Evaluating document embeddings for multilingual models on Scandinavian languages</li> <li>Allow ranking of competing Scandinavian and multilingual models using no more compute that what a consumer laptop can provide </li> </ul> AllDanishNorwegianSwedish <p></p> <p> </p> <p></p> <p></p>"},{"location":"#comparison-to-other-benchmarks","title":"Comparison to other benchmarks","text":"<p>If you use this benchmark for a relative ranking of language models you should also take a look at ScandEval, which as opposed the this benchmark fully fine-tunes the models. It also includes structured predictions tasks such as named entity recognition. Many of the tasks in this embeddings benchmark is also included in ScandEval. A notable difference between the ScandEval and this benchmark is that it does not include machine translated tasks.</p> <p>The tasks within this benchmark is also included in the MTEB leaderboard, though the aggregations methods very slightly. The MTEB is primarily an English embedding benchmark, with a few multilingual tasks along with a few additional languages. As a part of this project the tasks was also added to the MTEB leaderboard.</p>"},{"location":"getting_started/","title":"Getting started","text":"In\u00a0[6]: Copied! <pre>import seb\n\ntasks = [\"DKHate\"]\nmodel = seb.get_model(\"jonfd/electra-small-nordic\")\n\n# initialize benchmark with tasks\nbenchmark = seb.Benchmark(tasks=tasks)\n\n# benchmark the model\nbenchmark_result = benchmark.evaluate_model(model)\n</pre> import seb  tasks = [\"DKHate\"] model = seb.get_model(\"jonfd/electra-small-nordic\")  # initialize benchmark with tasks benchmark = seb.Benchmark(tasks=tasks)  # benchmark the model benchmark_result = benchmark.evaluate_model(model) In\u00a0[7]: Copied! <pre>benchmark_result  # examine output\n</pre> benchmark_result  # examine output Out[7]: <pre>BenchmarkResults(meta=ModelMeta(name='electra-small-nordic', description=None, huggingface_name='jonfd/electra-small-nordic', reference='https://huggingface.co/{hf_name}', languages=['da', 'no', 'sv']), task_results=[TaskResult(task_name='DKHate', task_description='Danish Tweets annotated for Hate Speech either being Offensive or not', task_version='59d12749a3c91a186063c7d729ec392fda94681c_1.0.3.dev0', time_of_run=datetime.datetime(2023, 7, 27, 13, 21, 43, 861342), scores={'da': {'accuracy': 0.5945288753799393, 'f1': 0.4912211182797449, 'ap': 0.15442320525050762, 'accuracy_stderr': 0.07818347662767612, 'f1_stderr': 0.05511334661624392, 'ap_stderr': 0.019081572459727296, 'main_score': 0.5945288753799393}}, main_score='accuracy')])</pre> In\u00a0[8]: Copied! <pre>benchmark_result[0] # examine the results for the first task\n</pre> benchmark_result[0] # examine the results for the first task Out[8]: <pre>TaskResult(task_name='DKHate', task_description='Danish Tweets annotated for Hate Speech either being Offensive or not', task_version='59d12749a3c91a186063c7d729ec392fda94681c_1.0.3.dev0', time_of_run=datetime.datetime(2023, 7, 27, 13, 21, 43, 861342), scores={'da': {'accuracy': 0.5945288753799393, 'f1': 0.4912211182797449, 'ap': 0.15442320525050762, 'accuracy_stderr': 0.07818347662767612, 'f1_stderr': 0.05511334661624392, 'ap_stderr': 0.019081572459727296, 'main_score': 0.5945288753799393}}, main_score='accuracy')</pre> In\u00a0[1]: Copied! <pre>import seb\n\nmodel_name = \"sentence-transformers/all-MiniLM-L12-v2\"\n\ndef get_my_model():\n    from sentence_transformers import SentenceTransformer\n\n    return SentenceTransformer(model_name)\n\n\n@seb.models.register(model_name)\ndef create_all_mini_lm_l6_v2() -&gt; seb.SebModel:\n    hf_name = model_name\n\n    # create meta data\n    meta = seb.ModelMeta(\n        name=hf_name.split(\"/\")[-1],\n        huggingface_name=hf_name,\n        reference=\"https://huggingface.co/{hf_name}\",\n        languages=[],\n    )\n    return seb.SebModel(\n        loader=get_my_model,\n        meta=meta,\n    )\n</pre> import seb  model_name = \"sentence-transformers/all-MiniLM-L12-v2\"  def get_my_model():     from sentence_transformers import SentenceTransformer      return SentenceTransformer(model_name)   @seb.models.register(model_name) def create_all_mini_lm_l6_v2() -&gt; seb.SebModel:     hf_name = model_name      # create meta data     meta = seb.ModelMeta(         name=hf_name.split(\"/\")[-1],         huggingface_name=hf_name,         reference=\"https://huggingface.co/{hf_name}\",         languages=[],     )     return seb.SebModel(         loader=get_my_model,         meta=meta,     ) In\u00a0[\u00a0]: Copied! <pre># deliberately not running this test as it takes a while\nfrom seb import run_benchmark\n\nresults = run_benchmark()\n</pre> # deliberately not running this test as it takes a while from seb import run_benchmark  results = run_benchmark() <p>This runs the full benchmark on all the registrered models as well as all the registrered datasets. The results are returned as a dictionary of where the keys represent the benchmark and values are a list of benchmark results.</p>"},{"location":"getting_started/#getting-started","title":"Getting started\u00b6","text":"<p>This is a minimal documentation at the moment at I am unsure how many will use the package. If you do want to use the package, but feel like the documentation is lacking feel free to open an issue on GitHub.</p>"},{"location":"getting_started/#running-a-task","title":"Running a task\u00b6","text":"<p>To run a task you will need to fetch the task, a model run it.</p>"},{"location":"getting_started/#adding-a-model","title":"Adding a model\u00b6","text":"<p>The benchmark uses a registry to add models. A model in <code>seb</code> includes two thing. 1) a metadata object (<code>seb.ModelMeta</code>) describing the metadata of the model and 2) a loader for the model itself, which is an object that needs an encode methods as described by the <code>seb.ModelInterface</code>. Here is an example of how to add a model:</p>"},{"location":"getting_started/#reproducing-the-benchmark","title":"Reproducing the Benchmark\u00b6","text":"<p>Reproducing the benchmark is easy and is doable simply using the following command:</p>"},{"location":"installation/","title":"Installation","text":"<p>You can install the <code>seb</code> via pip from PyPI:</p> <pre><code>pip install seb\n</code></pre> <p>or from GitHub using:</p> <pre><code>pip install git+https://github.com/KennethEnevoldsen/scandinavian-embedding-benchmark\n</code></pre>"},{"location":"run_benchmark/","title":"Run benchmark","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"\nScript for running the benchmark and pushing the results to Datawrapper.\n\nExample:\n    python run_benchmark.py --data-wrapper-api-token &lt;token&gt;\n\"\"\"\n</pre> \"\"\" Script for running the benchmark and pushing the results to Datawrapper.  Example:          python run_benchmark.py --data-wrapper-api-token  \"\"\" In\u00a0[\u00a0]: Copied! <pre>import argparse\nfrom typing import Dict, List\n</pre> import argparse from typing import Dict, List In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport pandas as pd\nfrom datawrapper import Datawrapper\n</pre> import numpy as np import pandas as pd from datawrapper import Datawrapper In\u00a0[\u00a0]: Copied! <pre>import seb\nfrom seb.full_benchmark import BENCHMARKS\n</pre> import seb from seb.full_benchmark import BENCHMARKS In\u00a0[\u00a0]: Copied! <pre>subset_to_chart_id = {\n    \"Mainland Scandinavian\": \"7Nwjx\",\n    \"Danish\": \"us1YK\",\n    \"Norwegian\": \"pV87q\",\n    \"Swedish\": \"aL23t\",\n}\n</pre> subset_to_chart_id = {     \"Mainland Scandinavian\": \"7Nwjx\",     \"Danish\": \"us1YK\",     \"Norwegian\": \"pV87q\",     \"Swedish\": \"aL23t\", } In\u00a0[\u00a0]: Copied! <pre>datawrapper_lang_codes = {\n    \"da\": \"dk\",\n    \"nb\": \"no\",\n    \"sv\": \"se\",\n}\n</pre> datawrapper_lang_codes = {     \"da\": \"dk\",     \"nb\": \"no\",     \"sv\": \"se\", } In\u00a0[\u00a0]: Copied! <pre>def get_main_score(task: seb.TaskResult, langs: List[str]) -&gt; float:\n    _langs = set(langs) &amp; set(task.languages)\n    return task.get_main_score(_langs) * 100\n</pre> def get_main_score(task: seb.TaskResult, langs: List[str]) -&gt; float:     _langs = set(langs) &amp; set(task.languages)     return task.get_main_score(_langs) * 100 In\u00a0[\u00a0]: Copied! <pre>def create_mdl_name(mdl: seb.ModelMeta):\n    reference = mdl.reference\n    name = mdl.name\n\n    if reference:\n        mdl_name = f\"[{name}]({reference})\"\n    else:\n        mdl_name = name\n\n    if mdl.languages:\n        lang_code = \" \".join(\n            [\n                f\":{datawrapper_lang_codes[l]}:\"\n                for l in mdl.languages\n                if l in datawrapper_lang_codes\n            ]\n        )\n        mdl_name = f\"{mdl_name} {lang_code}\"\n\n    return mdl_name\n</pre> def create_mdl_name(mdl: seb.ModelMeta):     reference = mdl.reference     name = mdl.name      if reference:         mdl_name = f\"[{name}]({reference})\"     else:         mdl_name = name      if mdl.languages:         lang_code = \" \".join(             [                 f\":{datawrapper_lang_codes[l]}:\"                 for l in mdl.languages                 if l in datawrapper_lang_codes             ]         )         mdl_name = f\"{mdl_name} {lang_code}\"      return mdl_name In\u00a0[\u00a0]: Copied! <pre>def benchmark_result_to_row(\n    result: seb.BenchmarkResults, langs: List[str]\n) -&gt; pd.DataFrame:\n    mdl_name = create_mdl_name(result.meta)\n    # sort by task name\n    task_results = result.task_results\n    sorted_tasks = sorted(task_results, key=lambda t: t.task_name)\n    task_names = [t.task_name for t in sorted_tasks]\n    scores = [get_main_score(t, langs) for t in sorted_tasks] # type: ignore\n\n    df = pd.DataFrame([scores], columns=task_names, index=[mdl_name])\n    df[\"Average\"] = np.mean(scores)  # type: ignore\n    return df\n</pre> def benchmark_result_to_row(     result: seb.BenchmarkResults, langs: List[str] ) -&gt; pd.DataFrame:     mdl_name = create_mdl_name(result.meta)     # sort by task name     task_results = result.task_results     sorted_tasks = sorted(task_results, key=lambda t: t.task_name)     task_names = [t.task_name for t in sorted_tasks]     scores = [get_main_score(t, langs) for t in sorted_tasks] # type: ignore      df = pd.DataFrame([scores], columns=task_names, index=[mdl_name])     df[\"Average\"] = np.mean(scores)  # type: ignore     return df In\u00a0[\u00a0]: Copied! <pre>def convert_to_table(results: List[seb.BenchmarkResults], langs: List[str]):\n    rows = [benchmark_result_to_row(result, langs) for result in results]\n    df = pd.concat(rows)\n    df = df.sort_values(by=\"Average\", ascending=False)\n\n    # ensure that the average first column\n    cols = df.columns.tolist()\n    cols = cols[-1:] + cols[:-1]\n    df = df[cols]\n\n    # convert name to column\n    df = df.reset_index()\n    df = df.rename(columns={\"index\": \"Model\"})\n\n    return df\n</pre> def convert_to_table(results: List[seb.BenchmarkResults], langs: List[str]):     rows = [benchmark_result_to_row(result, langs) for result in results]     df = pd.concat(rows)     df = df.sort_values(by=\"Average\", ascending=False)      # ensure that the average first column     cols = df.columns.tolist()     cols = cols[-1:] + cols[:-1]     df = df[cols]      # convert name to column     df = df.reset_index()     df = df.rename(columns={\"index\": \"Model\"})      return df In\u00a0[\u00a0]: Copied! <pre>def push_to_datawrapper(df: pd.DataFrame, chart_id: str, token: str):\n    dw = Datawrapper(access_token=token)\n    assert dw.account_info(), \"Could not connect to Datawrapper\"\n    resp = dw.add_data(chart_id, data=df)\n    assert 200 &lt;= resp.status_code &lt; 300, \"Could not add data to Datawrapper\"\n    iframe_html = dw.publish_chart(chart_id)\n    assert iframe_html, \"Could not publish chart\"\n</pre> def push_to_datawrapper(df: pd.DataFrame, chart_id: str, token: str):     dw = Datawrapper(access_token=token)     assert dw.account_info(), \"Could not connect to Datawrapper\"     resp = dw.add_data(chart_id, data=df)     assert 200 &lt;= resp.status_code &lt; 300, \"Could not add data to Datawrapper\"     iframe_html = dw.publish_chart(chart_id)     assert iframe_html, \"Could not publish chart\" In\u00a0[\u00a0]: Copied! <pre>def main(data_wrapper_api_token: str):\n    results = seb.run_benchmark(use_cache=True)\n\n    for subset, result in results.items():\n        langs = BENCHMARKS[subset]\n\n        table = convert_to_table(result, langs)\n        chart_id = subset_to_chart_id[subset]\n        push_to_datawrapper(table, chart_id, data_wrapper_api_token)\n</pre> def main(data_wrapper_api_token: str):     results = seb.run_benchmark(use_cache=True)      for subset, result in results.items():         langs = BENCHMARKS[subset]          table = convert_to_table(result, langs)         chart_id = subset_to_chart_id[subset]         push_to_datawrapper(table, chart_id, data_wrapper_api_token) In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--data-wrapper-api-token\",\n        type=str,\n        required=True,\n        help=\"Datawrapper API token\",\n    )\n\n    args = parser.parse_args()\n    main(args.data_wrapper_api_token)\n</pre> if __name__ == \"__main__\":     parser = argparse.ArgumentParser()     parser.add_argument(         \"--data-wrapper-api-token\",         type=str,         required=True,         help=\"Datawrapper API token\",     )      args = parser.parse_args()     main(args.data_wrapper_api_token)"}]}